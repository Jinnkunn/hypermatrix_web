@book{ruby,
   title     = {The Ruby Programming Language},
   author    = {Flanagan, David and Matsumoto, Yukihiro},
   year      = {2008},
   publisher = {O'Reilly Media}
}

@book{smalltalk,
   title     = {Smalltalk Best Practice Patterns},
   author    = {Kent Beck},
   year      = {1996},
   publisher = {Prentice Hall}
}

@inproceedings{sajjad-etal-2022-effect,
    title = {Effect of Post-processing on Contextualized Word Representations},
    author = {Hassan Sajjad and Firoj Alam and Fahim Dalvi and Nadir Durrani},
    url = {https://aclanthology.org/2022.coling-1.277.pdf},
    year = {2022},
    date = {2022-10-01},
    urldate = {2022-10-01},
    booktitle = {Proceedings of the 29th International Conference on Computational Linguistics},
    pages = {3127--3142},
    publisher = {International Committee on Computational Linguistics},
    address = {Gyeongju, Republic of Korea},
    abstract = {Post-processing of static embedding has been shown to improve their performance on both lexical and sequence-level tasks. However, post-processing for contextualized embeddings is an under-studied problem. In this work, we question the usefulness of post-processing for contextualized embeddings obtained from different layers of pre-trained language models. More specifically, we standardize individual neuron activations using z-score, min-max normalization, and by removing top principal components using the all-but-the-top method. Additionally, we apply unit length normalization to word representations. On a diverse set of pre-trained models, we show that post-processing unwraps vital information present in the representations for both lexical tasks (such as word similarity and analogy) and sequence classification tasks. Our findings raise interesting points in relation to the research studies that use contextualized representations, and suggest z-score normalization as an essential step to consider when using them in an application.},
    keywords = {NLP},
    pubstate = {published},
    tppubtype = {inproceedings}
}